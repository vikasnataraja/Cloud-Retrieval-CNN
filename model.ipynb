{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/viha4393/anaconda36/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import MaxPooling2D, AveragePooling2D, UpSampling2D\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization, Activation, Dropout\n",
    "from keras.layers import ZeroPadding2D, Lambda\n",
    "\n",
    "# might be keras.layers.merge import Add Concatenate\n",
    "from keras.layers import Concatenate, Add\n",
    "from keras.models import Model\n",
    "#from keras.layers.merge import Concatenate, Add\n",
    "\n",
    "def BatchNorm():\n",
    "    return BatchNormalization(momentum=0.95, epsilon=1e-5)\n",
    "\n",
    "def common_skip(prev, num_filters, kernel_size, \n",
    "                stride_tuple, pad_type, atrous_rate, name):\n",
    "    \"\"\"\n",
    "    The common ResNet block shared by the identity block\n",
    "    and the convolutional block. Both of those blocks share\n",
    "    this common functionality.\n",
    "    \"\"\"\n",
    "\n",
    "    prev = BatchNorm()(prev)\n",
    "    #print('bn',prev.shape)\n",
    "    prev = Activation('relu')(prev)\n",
    "    #print('type',type(prev))\n",
    "    #print('actv',prev.shape)\n",
    "    # prev = ZeroPadding2D(padding=(pad, pad))(prev)\n",
    "    \n",
    "    \"\"\"Syntax:\n",
    "    keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), \n",
    "                    padding='valid', data_format=None, dilation_rate=(1, 1),\n",
    "                    activation=None, use_bias=True)\n",
    "    \"\"\"\n",
    "    x1 = Conv2D(filters=num_filters, kernel_size=kernel_size, \n",
    "                strides=stride_tuple, dilation_rate=atrous_rate,\n",
    "                padding=pad_type, use_bias=False)(prev)\n",
    "    if name==\"halve_feature_map\":\n",
    "        x1 = MaxPooling2D(pool_size=(2,2),padding='same')(x1)\n",
    "\n",
    "    x1 = BatchNorm()(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    # dropout rate is 10%\n",
    "    x1 = Dropout(rate=0.1)(x1)\n",
    "    x2 = Conv2D(num_filters, kernel_size=kernel_size, \n",
    "                  strides=stride_tuple, dilation_rate=atrous_rate,\n",
    "                  padding=pad_type, use_bias=False)(x1)\n",
    "    #print('common_skip end',x2.shape)\n",
    "    # I think this should be followed by BatchNorm and an activation\n",
    "    return x2\n",
    "\n",
    "def convolution_branch(name, num_filters, kernel_size, \n",
    "                       stride_tuple, pad_type, atrous_rate, prev):\n",
    "\n",
    "    prev = Conv2D(num_filters, kernel_size=kernel_size, strides=stride_tuple,\n",
    "                  padding=pad_type, dilation_rate=atrous_rate, use_bias=False)(prev)\n",
    "    \n",
    "    if name=='halve_feature_map':\n",
    "        # halve the size of feature map by using same padding, 2x2 pooling\n",
    "        prev = MaxPooling2D(pool_size=(2,2),padding='same')(prev)\n",
    "\n",
    "    prev = BatchNorm()(prev)\n",
    "    return prev\n",
    "\n",
    "\n",
    "def empty_branch(prev):\n",
    "    return prev\n",
    "\n",
    "def convolutional_resnet_block(prev_layer, num_filters, name, kernel_size,\n",
    "                               stride_tuple, pad_type, atrous_rate=1):\n",
    "\n",
    "    #prev_layer = Activation('relu')(prev_layer)\n",
    "    block_1 = common_skip(prev=prev_layer, num_filters=num_filters, \n",
    "                          name=name, kernel_size=kernel_size, \n",
    "                          stride_tuple=stride_tuple,\n",
    "                          pad_type=pad_type,\n",
    "                          atrous_rate=atrous_rate)\n",
    "\n",
    "    block_2 = convolution_branch(name, num_filters=num_filters,\n",
    "                                 kernel_size=kernel_size, \n",
    "                                 stride_tuple=stride_tuple,\n",
    "                                 prev=prev_layer, \n",
    "                                 pad_type=pad_type,\n",
    "                                 atrous_rate=atrous_rate)\n",
    "    #print('conv_block',block_1.shape,block_2.shape)\n",
    "    added = Add()([block_1, block_2])\n",
    "    \n",
    "    return added\n",
    "    \n",
    "\n",
    "def identity_resnet_block(prev_layer, num_filters, name, kernel_size,\n",
    "                          stride_tuple, pad_type, atrous_rate=1):\n",
    "    \n",
    "    #prev_layer = Activation('relu')(prev_layer)\n",
    "    #print('activ',prev_layer.shape)\n",
    "    block_1 = common_skip(prev=prev_layer, num_filters=num_filters, \n",
    "                          name=name, kernel_size=kernel_size, \n",
    "                          stride_tuple=stride_tuple,\n",
    "                          pad_type=pad_type, \n",
    "                          atrous_rate=atrous_rate)\n",
    "    \n",
    "    block_2 = empty_branch(prev_layer)\n",
    "    #print(block_1.shape,block_2.shape)\n",
    "    added = Add()([block_1, block_2])\n",
    "    \n",
    "    return added\n",
    "\n",
    "\n",
    "def ResNet(input_layer):\n",
    "    #print('inp',input_layer.shape)\n",
    "    x = Conv2D(16, (3, 3), strides=(1, 1), padding='same',\n",
    "                  use_bias=False)(input_layer)\n",
    "    #print('conv',input_layer.shape)\n",
    "    x = identity_resnet_block(x, num_filters=16, kernel_size=(3,3),\n",
    "                              stride_tuple=(1,1), name=\"identity\",\n",
    "                              pad_type='same', atrous_rate=1)\n",
    "    \n",
    "    x = convolutional_resnet_block(x, num_filters=32, kernel_size=(3,3),\n",
    "                                   stride_tuple=(1,1), name=\"halve_feature_map\",\n",
    "                                   pad_type='same', atrous_rate=1)\n",
    "    \n",
    "    x = convolutional_resnet_block(x, num_filters=64, kernel_size=(3,3),\n",
    "                                   stride_tuple=(1,1), name=\"halve_feature_map\", \n",
    "                                   pad_type='same', atrous_rate=1)\n",
    "    \n",
    "    x = identity_resnet_block(x, num_filters=64, kernel_size=(3,3),\n",
    "                              stride_tuple=(1,1), name=\"identity\",\n",
    "                              pad_type='same', atrous_rate=1)\n",
    "    \n",
    "    x = convolutional_resnet_block(x, num_filters=128, kernel_size=(3,3),\n",
    "                                   stride_tuple=(1,1), name=\"halve_feature_map\", \n",
    "                                   pad_type='same', atrous_rate=1)\n",
    "    \n",
    "    x = identity_resnet_block(x, num_filters=128, kernel_size=(3,3),\n",
    "                              stride_tuple=(1,1), name=\"identity\",\n",
    "                              pad_type='same', atrous_rate=1)\n",
    "    \n",
    "    \"\"\" dilated/atrous convolutional ResNet block starts here\"\"\"\n",
    "    \n",
    "    x = convolutional_resnet_block(x, num_filters=256, kernel_size=(3,3),\n",
    "                                   stride_tuple=(1,1), name=\"full_feature_map\", \n",
    "                                   pad_type='same', atrous_rate=2)\n",
    "    \n",
    "    x = identity_resnet_block(x, num_filters=256, kernel_size=(3,3),\n",
    "                              stride_tuple=(1,1), name=\"identity\",\n",
    "                              pad_type='same', atrous_rate=2)\n",
    "    \n",
    "    x = convolutional_resnet_block(x, num_filters=512, kernel_size=(3,3),\n",
    "                                   stride_tuple=(1,1), name=\"full_feature_map\", \n",
    "                                   pad_type='same', atrous_rate=4)\n",
    "    \n",
    "    x = identity_resnet_block(x, num_filters=512, kernel_size=(3,3),\n",
    "                              stride_tuple=(1,1), name=\"identity\",\n",
    "                              pad_type='same', atrous_rate=4)\n",
    "    #Syntax:\n",
    "    #keras.layers.Conv2D(filters, kernel_size, strides=(1, 1), \n",
    "    #                padding='valid', data_format=None, dilation_rate=(1, 1),\n",
    "    #                activation=None, use_bias=True)\n",
    "    \n",
    "    x = Conv2D(filters=512,kernel_size=(3,3),\n",
    "               strides=(1,1), padding='same',dilation_rate=2)(x)\n",
    "    x = BatchNorm()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=512,kernel_size=(3,3),\n",
    "               strides=(1,1), padding='same',dilation_rate=2)(x)\n",
    "    x = BatchNorm()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    \"\"\"End of dilated convolution block\"\"\"\n",
    "    \n",
    "    x = Conv2D(filters=512,kernel_size=(3,3),\n",
    "               strides=(1,1), padding='same',dilation_rate=1)(x)\n",
    "    x = BatchNorm()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(filters=512,kernel_size=(3,3),\n",
    "               strides=(1,1), padding='same',dilation_rate=1)(x)\n",
    "    x = BatchNorm()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    print('Finished building ResNet')\n",
    "    return x\n",
    "    \n",
    "\"\"\"Spatial Pyramid Pooling\"\"\"\n",
    "\n",
    "def upsample_bilinear(in_tensor, new_size):\n",
    "    resized_height, resized_width = new_size\n",
    "    return tf.image.resize(images=in_tensor,size=[resized_height,resized_width],\n",
    "                           method='bilinear',\n",
    "                           align_corners=True)\n",
    "\n",
    "def spp_block(prev_layer, pool_size, feature_map_shape):\n",
    "\n",
    "    #kernel = [(1,1),(2,2),(4,4),(8,8)]\n",
    "    #strides = [(1,1),(2,2),(4,4),(8,8)]\n",
    "    pool_size_tuple = (pool_size, pool_size)\n",
    "    pool_layer = AveragePooling2D(pool_size=pool_size_tuple)(prev_layer)\n",
    "    conv1 = Conv2D(128, (1, 1), strides=(1, 1),\n",
    "                        use_bias=False)(pool_layer)\n",
    "    conv1 = BatchNorm()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    \n",
    "    # upsampled_layer = Lambda(Interp, arguments={\n",
    "    #                    'shape': feature_map_shape})(prev_layer)\n",
    "    \n",
    "    # upsampling\n",
    "    upsampled_layer = upsample_bilinear(conv1,feature_map_shape)\n",
    "    # prev_layer = UpSampling2D(size=(2, 2),\n",
    "    #                          interpolation='bilinear')\n",
    "\n",
    "    return upsampled_layer\n",
    "\n",
    "def build_pyramid_pooling_module(resnet_last):\n",
    "    \"\"\"Build the Pyramid Pooling Module.\"\"\"\n",
    "    \n",
    "    # feature map size to be used for interpolation\n",
    "    feature_map_size = (16,16) # (height, width) not (width, height)\n",
    "    pool_sizes = [1,2,4,8]\n",
    "\n",
    "    pool_block1 = spp_block(resnet_last, pool_sizes[0], feature_map_size)\n",
    "    pool_block2 = spp_block(resnet_last, pool_sizes[1], feature_map_size)\n",
    "    pool_block4 = spp_block(resnet_last, pool_sizes[2], feature_map_size)\n",
    "    pool_block8 = spp_block(resnet_last, pool_sizes[3], feature_map_size)\n",
    "\n",
    "    # concat all these layers. resulted\n",
    "    # shape=(1,feature_map_size_x,feature_map_size_y,4096)\n",
    "    concat = Concatenate()([resnet_last,\n",
    "                            pool_block8,\n",
    "                            pool_block4,\n",
    "                            pool_block2,\n",
    "                            pool_block1])\n",
    "    return concat    \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Deconvolution layer comes after concatenation of SPP layers\n",
    "https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/Conv2DTranspose\n",
    "From the paper:\n",
    "\"Finally, multi-scale features are fused to obtain an image with \n",
    "the same size as the input image by the transposed convolution\"\n",
    "\"\"\"\n",
    "def add_deconvolution_layer(concat_layer):\n",
    "    \"\"\"\n",
    "    keras.layers.Conv2DTranspose(filters, kernel_size,\n",
    "                                 strides=(1, 1), padding='valid', \n",
    "                                 output_padding=None, data_format=None, \n",
    "                                 dilation_rate=(1, 1), \n",
    "                                 activation=None, use_bias=True)\n",
    "\n",
    "    \"\"\"\n",
    "    print('concat',concat_layer.shape)\n",
    "    deconv_layer = Conv2DTranspose(filters=1,kernel_size=(16,16),\n",
    "                                   strides=(1,1))(concat_layer)\n",
    "    print(deconv_layer.shape)\n",
    "    #deconv_layer = tf.reshape(deconv_layer,(128,128,1))\n",
    "    deconv_layer.set_shape((None,128,128,1))\n",
    "    print(deconv_layer.shape)\n",
    "    deconv_layer = BatchNorm()(deconv_layer)\n",
    "    deconv_layer = Activation('softmax')(deconv_layer)\n",
    "    return deconv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "#from train import build_PSPNet_model, train_val_generator, train_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Input\n",
    "train_im_list = os.listdir('./data/train_images/')\n",
    "train,val = train_test_split(train_im_list,test_size=0.20)\n",
    "\n",
    "from ImageDataGenerator import ImageGenerator\n",
    "\n",
    "train_gen = ImageGenerator(image_dir='./data/train_images/',\n",
    "                           anno_dir='./data/labels/',\n",
    "                           mode='train')\n",
    "val_gen = ImageGenerator(image_dir='./data/train_images/',\n",
    "                           anno_dir='./data/labels/',\n",
    "                           mode='validation')\n",
    "input_shape = (128,128)\n",
    "num_channels=3\n",
    "input_layer = Input(shape=(input_shape[0],input_shape[1],num_channels))\n",
    "resnet_block = ResNet(input_layer)\n",
    "#model = build_PSPNet_model(input_shape=(128,128),\n",
    "#                           num_channels=3,\n",
    "#                           loss_function='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp_block = build_pyramid_pooling_module(resnet_block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spp_block.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconv_layer = add_deconvolution_layer(spp_block)\n",
    "#deconv_layer = Lambda(add_deconvolution_layer)(spp_block)\n",
    "print(deconv_layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(deconv_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deconv_layer.set_shape((None,128,128,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_layer,outputs=deconv_layer)\n",
    "\n",
    "adam = Adam(learning_rate=lr)\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss=loss_function,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
